{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune Pix2Struct on a key-value pair dataset\n\nIn this notebook, we'll fine-tune Google's [Pix2Struct](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct) model on the [CORD dataset](https://huggingface.co/datasets/naver-clova-ix/cord-v2), in the format in which the Donut authors (Donut is a model very similar to Pix2Struct in terms of architecture) prepared it. See also my [notebook](#) regarding preparing a custom dataset in this format.\n\nThe goal for the model is to predict a piece of text given a document image.\n\n## Set-up environment\n\nWe start by installing ðŸ¤— Transformers and ðŸ¤— Datasets.","metadata":{"id":"g-omEyoV-DJm"}},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git","metadata":{"id":"w8m7YDPbfxtV","outputId":"5437720f-a8f0-4ba9-f254-2879397d5e0b","execution":{"iopub.status.busy":"2023-07-20T11:28:35.748160Z","iopub.execute_input":"2023-07-20T11:28:35.748758Z","iopub.status.idle":"2023-07-20T11:29:20.055111Z","shell.execute_reply.started":"2023-07-20T11:28:35.748713Z","shell.execute_reply":"2023-07-20T11:29:20.054025Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31mÃ—\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-dngqqfw_\u001b[0m did not run successfully.\n  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m fatal: unable to access 'https://github.com/huggingface/transformers.git/': Could not resolve host: github.com\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n\n\u001b[31mÃ—\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-dngqqfw_\u001b[0m did not run successfully.\n\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n\u001b[31mâ•°â”€>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q datasets","metadata":{"id":"qriqrURi93k_","outputId":"b8564f9c-9d17-4c69-bd3e-eceef756acb8","execution":{"iopub.status.busy":"2023-07-20T11:29:20.060409Z","iopub.execute_input":"2023-07-20T11:29:20.062566Z","iopub.status.idle":"2023-07-20T11:29:24.026965Z","shell.execute_reply.started":"2023-07-20T11:29:20.062511Z","shell.execute_reply":"2023-07-20T11:29:24.025866Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"We'll also install PyTorch Lightning and Weights and Biases, as those are the tools we'll use for training.","metadata":{"id":"SZKAVq-6_tvv"}},{"cell_type":"code","source":"!pip install -q lightning wandb","metadata":{"id":"oiJdqe_G3Ctc","outputId":"6d0ee526-623c-42a7-ff30-e8d6dcedd660","execution":{"iopub.status.busy":"2023-07-20T11:29:24.028969Z","iopub.execute_input":"2023-07-20T11:29:24.033627Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ea3dbbab6d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/lightning/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load dataset\n\nLet's load the dataset from the [hub](https://huggingface.co/datasets/naver-clova-ix/cord-v2).","metadata":{"id":"cliqx5Bn-FBC"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"naver-clova-ix/cord-v2\")","metadata":{"id":"WPcaDeSY-C3L","outputId":"71eb75b9-973a-4e0d-8f1c-88b0a4f1a4e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset consists of 3 splits as can be seen: train, validation and test. Each example consists of an \"image\" and a \"ground_truth\" string.","metadata":{"id":"E9wkxgDlulRA"}},{"cell_type":"code","source":"dataset","metadata":{"id":"Zp7RiX9B-Yeh","outputId":"b5027cd7-b85f-4691-fea4-b77478617cd5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check and visualize the first training example:","metadata":{"id":"5SmYAHpSuoar"}},{"cell_type":"code","source":"example = dataset['train'][0]\nimage = example['image']\n# let's make the image a bit smaller when visualizing\nwidth, height = image.size\ndisplay(image.resize((int(width*0.3), int(height*0.3))))","metadata":{"id":"1T-gdEA_-K90","outputId":"3ef90fce-a585-4b01-e951-7fb32bc04028","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's load the corresponding JSON dictionary (as string representation)\nground_truth = example['ground_truth']\nprint(ground_truth)","metadata":{"id":"NFFju745-Y0J","outputId":"b04a1e31-244d-40ac-b6b7-1b5288e8d232","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Python's `literal_eval` function, you can turn it into an actual Python dictionary:","metadata":{"id":"sVvw9bocuxQ7"}},{"cell_type":"code","source":"from ast import literal_eval\n\nliteral_eval(ground_truth)['gt_parse']","metadata":{"id":"qsRNIuLk-ccZ","outputId":"26e84d57-b8e6-4dc0-f9a1-fb023c7f870d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model and processor\n\nNext, let's load the model and its processor from the [hub](https://huggingface.co/models?other=pix2struct). Here we just use the pre-trained only, base-sized model, but note that there are 20 different checkpoints released on the hub.","metadata":{"id":"23_XHpOJ-kKp"}},{"cell_type":"code","source":"from transformers import Pix2StructForConditionalGeneration, AutoProcessor\n\nrepo_id = \"google/pix2struct-base\"\n\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = Pix2StructForConditionalGeneration.from_pretrained(repo_id, is_encoder_decoder=True)","metadata":{"id":"KdsGlgt8-lU3","outputId":"4e7b1871-e933-4643-9bc0-f329a9eeab90","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create PyTorch dataset\n\nWe create a regular PyTorch Dataset class which returns examples of the data, prepared for the model.","metadata":{"id":"kx75D66l_XWN"}},{"cell_type":"code","source":"import json\nimport random\nfrom typing import Any, List\nfrom torch.utils.data import Dataset\n\nadded_tokens = []\n\nclass ImageCaptioningDataset(Dataset):\n    def __init__(\n        self,\n        dataset_name_or_path: str,\n        max_patches: int = 1024,\n        max_length: int = 512,\n        split: str = \"train\",\n        ignore_id: int = -100,\n        task_start_token: str = \"\",\n        prompt_end_token: str = None,\n        sort_json_key: bool = True,\n    ):\n        super().__init__()\n\n        self.split = split\n        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n        self.max_patches = max_patches\n        self.max_length = max_length\n        self.ignore_id = ignore_id\n        self.task_start_token = task_start_token\n        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n        self.sort_json_key = sort_json_key\n\n        self.gt_token_sequences = []\n        for ground_truth in self.dataset[\"ground_truth\"]:\n            ground_truth = json.loads(ground_truth)\n            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n                assert isinstance(ground_truth[\"gt_parses\"], list)\n                gt_jsons = ground_truth[\"gt_parses\"]\n            else:\n                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n                gt_jsons = [ground_truth[\"gt_parse\"]]\n\n            self.gt_token_sequences.append(\n                [\n                    self.json2token(\n                        gt_json,\n                        update_special_tokens_for_json_key=self.split == \"train\",\n                        sort_json_key=self.sort_json_key,\n                    )\n                    for gt_json in gt_jsons  # load json from list of json\n                ]\n            )\n\n        self.add_tokens([self.task_start_token, self.prompt_end_token])\n        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n\n    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n        \"\"\"\n        Convert an ordered JSON object into a token sequence\n        \"\"\"\n        if type(obj) == dict:\n            if len(obj) == 1 and \"text_sequence\" in obj:\n                return obj[\"text_sequence\"]\n            else:\n                output = \"\"\n                if sort_json_key:\n                    keys = sorted(obj.keys(), reverse=True)\n                else:\n                    keys = obj.keys()\n                for k in keys:\n                    if update_special_tokens_for_json_key:\n                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n                    output += (\n                        fr\"<s_{k}>\"\n                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n                        + fr\"</s_{k}>\"\n                    )\n                return output\n        elif type(obj) == list:\n            return r\"<sep/>\".join(\n                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n            )\n        else:\n            obj = str(obj)\n            if f\"<{obj}/>\" in added_tokens:\n                obj = f\"<{obj}/>\"  # for categorical special tokens\n            return obj\n    \n    def add_tokens(self, list_of_tokens: List[str]):\n        \"\"\"\n        Add special tokens to tokenizer and resize the token embeddings of the decoder\n        \"\"\"\n        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n        if newly_added_num > 0:\n            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n            added_tokens.extend(list_of_tokens)\n    \n    def __len__(self) -> int:\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n\n        # prepare inputs\n        encoding = processor(images=item[\"image\"], max_patches=self.max_patches, return_tensors=\"pt\")\n        encoding = {k:v.squeeze() for k,v in encoding.items()}\n        \n        # prepare targets\n        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n        input_ids = processor.tokenizer(\n            target_sequence,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        labels = input_ids.squeeze().clone()\n        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n        encoding[\"labels\"] = labels\n        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n        return encoding, target_sequence","metadata":{"id":"Z80qW5QD_R19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageCaptioningDataset(\"naver-clova-ix/cord-v2\",\n                                       split=\"train\", sort_json_key=False) # cord dataset is preprocessed, so no need for this\nval_dataset = ImageCaptioningDataset(\"naver-clova-ix/cord-v2\",\n                                       split=\"validation\", sort_json_key=False) # cord dataset is preprocessed, so no need for this","metadata":{"id":"R9wfda9WKzM5","outputId":"e6b854ec-b0c3-44bd-da2a-167aba138c44","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding, target_sequence = train_dataset[0]\nprint(encoding.keys())","metadata":{"id":"W0tU945QLYpS","outputId":"1bebe905-e2cf-49c3-9b03-01128a74c85d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(processor.decode([id.item() for id in encoding[\"labels\"] if id != -100]))","metadata":{"id":"K4xWnPqmLaJO","outputId":"f7697c2f-14f9-4422-cf04-ed1f2e3519c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(target_sequence)","metadata":{"id":"4NsqZcOqRznx","outputId":"06e9f89a-80ab-4932-d3ea-72a76d172759","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of added tokens:\", len(added_tokens))\nprint(added_tokens)","metadata":{"id":"nd7FSJsXhNeQ","outputId":"cc0ee98d-9288-4356-e4ab-b410d629fe9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(processor.tokenizer)","metadata":{"id":"gaFIRnptjx0D","outputId":"82fe3db4-10b2-402b-c899-fc5f22bc8f9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create PyTorch DataLoaders\n\nNext, we create PyTorch DataLoader which allow us to get batches of the data.","metadata":{"id":"rlu31GWsN25m"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport multiprocessing\n\nnum_cores = multiprocessing.cpu_count()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=num_cores)\nval_dataloader = DataLoader(val_dataset, batch_size=1, num_workers=num_cores)","metadata":{"id":"wGOTH8lGMxs1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get first batch\nbatch = next(iter(train_dataloader))\nencoding, target_sequences = batch","metadata":{"id":"w2fuLf2yOOLQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k,v in encoding.items():\n  print(k,v.shape)","metadata":{"id":"_-3hKE1zPF0g","outputId":"ea16cacb-109b-4305-8264-5d7012a9632f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(processor.batch_decode([id for id in encoding[\"labels\"].squeeze().tolist() if id != -100]))","metadata":{"id":"WNzgXbBRDVEn","outputId":"aafd6d0a-9627-44b4-d746-5a512b3dc783","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Lighting module\n\nAs we'll train the model using [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), we define a so-called LightningModule, which is an `nn.Module` with additional functionality such that we don't need to take care of device placement etc.","metadata":{"id":"5nRNyToFSyK2"}},{"cell_type":"code","source":"from pathlib import Path\nimport re\nfrom nltk import edit_distance\nimport numpy as np\nimport wandb\n\nimport torch\n\nfrom transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\n\nimport pytorch_lightning as pl\n\n\nclass Pix2Struct(pl.LightningModule):\n    def __init__(self, config, processor, model):\n        super().__init__()\n        self.config = config\n        self.processor = processor\n        self.model = model\n\n    def training_step(self, batch, batch_idx):\n        encoding, _ = batch\n        \n        outputs = self.model(**encoding)\n        loss = outputs.loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx, dataset_idx=0):\n        encoding, answers = batch\n        flattened_patches, attention_mask = encoding[\"flattened_patches\"], encoding[\"attention_mask\"]\n        batch_size = flattened_patches.shape[0]\n        # we feed the prompt to the model\n        decoder_input_ids = torch.full((batch_size, 1), self.model.config.text_config.decoder_start_token_id, device=self.device)\n        \n        outputs = self.model.generate(flattened_patches=flattened_patches,\n                                      attention_mask=attention_mask,\n                                      # decoder_input_ids=decoder_input_ids,\n                                      max_new_tokens=512,\n                                      return_dict_in_generate=True,)\n    \n        predictions = []\n        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n            # seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n            predictions.append(seq)\n\n        scores = []\n        for pred, answer in zip(predictions, answers):\n            # pred = re.sub(r\"(?:(?<=>) | (?=\", \"\", answer, count=1)\n            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n            \n            if self.config.get(\"verbose\", False) and len(scores) == 1:\n                print(f\"Prediction: {pred}\")\n                print(f\"    Answer: {answer}\")\n                print(f\" Normed ED: {scores[0]}\")\n\n        self.log(\"val_edit_distance\", np.mean(scores)) \n        \n        return scores\n\n    def configure_optimizers(self):\n        optimizer = Adafactor(self.parameters(), scale_parameter=False, relative_step=False, lr=self.config.get(\"lr\"), weight_decay=1e-05)\n        scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                    num_warmup_steps=self.config.get(\"num_warmup_steps\"),\n                                                    num_training_steps=self.config.get(\"max_steps\"))\n        \n        return [optimizer], [scheduler]\n\n    def train_dataloader(self):\n        return train_dataloader\n\n    def val_dataloader(self):\n        return val_dataloader","metadata":{"id":"kKcJw8xRSZnx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train!\n\nNext, we instantiate the model and define a PyTorch Lightning Trainer. We can specify all kinds of things, such as on which devices we'd like to train, whether to use gradient clipping, logging to Weights and Biases etc. Check the [docs](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer) for all arguments you can pass to the Trainer - there are a lot of them!\n\nNote: the model seems to converge **much slower** than Donut on this dataset. This may be due to a domain shift: Pix2Struct has been pre-trained on web page images (by predicting HTML on masked image portions). I haven't played too much with hyperparameters, I'm using the Adam optimizer with learning rate 1e-5. For reference, the authors use the AdaFactor optimizer with weight decay rate of 1e-5, with a learning rate that starts with 0.001 and goes up linearly to 0.01 for 1K steps and then a cosine decay for the rest of the training.\n\nUpdate: upgraded the notebook to use Adafactor with cosine decay learning rate schedule. Seeing faster (but still pretty slow) convergence.\n\nIf you find hyperparameters with great convergence, please share them by opening an issue on my [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials) repository.","metadata":{"id":"njOGca6cFTPq"}},{"cell_type":"code","source":"config = {\n          \"num_warmup_steps\": 1000,\n          \"max_steps\": 30000,\n          \"lr\": 0.01,\n          \"check_val_every_n_epoch\": 5,\n          \"gradient_clip_val\": 1.0,\n          \"warmup_steps\": 300, # 800/8*30/10, 10%\n          \"accumulate_grad_batches\": 8,\n          \"verbose\": True,\n          }\n\npl_module = Pix2Struct(config, processor, model)","metadata":{"id":"PtCXARL-Fijm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping\n\n#wandb.finish()\n#wandb_logger = WandbLogger(project=\"Pix2Struct\", name=\"demo-run-pix2struct-adafactor\")\n\n# trainer = pl.Trainer(\n#         accelerator=\"gpu\",\n#         devices=1,\n#         max_steps=config.get(\"max_steps\"),\n#         check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n#         gradient_clip_val=config.get(\"gradient_clip_val\"), # use gradient clipping\n#         accumulate_grad_batches=config.get(\"accumulate_grad_batches\"), # use gradient accumulation\n        \n# )\n\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    devices=1,\n    max_steps=config.get(\"max_steps\"),\n    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n    gradient_clip_val=config.get(\"gradient_clip_val\"),\n    accumulate_grad_batches=config.get(\"accumulate_grad_batches\")\n)","metadata":{"id":"MiXIlN1ATB9l","outputId":"46d47784-6747-421b-b557-2ac122281b9b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.fit(pl_module)","metadata":{"id":"qn_Y4JkoFZTj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nAfter training, you can load the model by getting the `pl_module.model` attribute from the PyTorch Ligthing module above (which returns the HuggingFace model). Then you can call either `save_pretrained` or `push_to_hub` on that object, and reload it back using `from_pretrained`. Refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) regarding inference code examples.","metadata":{"id":"jgoT-ebGhkJT"}},{"cell_type":"code","source":"print(\"Done\")","metadata":{"id":"vgeDopcjLl1x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}