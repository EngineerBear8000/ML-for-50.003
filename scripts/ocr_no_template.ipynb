{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8567382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import DetrFeatureExtractor\n",
    "from transformers import TableTransformerForObjectDetection\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import threading\n",
    "ocr_model = PaddleOCR(lang='en',use_angle_cls=False,show_log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfbbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(img,orig_img):\n",
    "    # Repeated Closing operation to remove text from the document.\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "    canny = cv2.Canny(img, 70, 300)\n",
    "    canny = cv2.dilate(canny, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)))\n",
    "    show(canny)\n",
    "    \n",
    "    # Finding contours for the detected edges.\n",
    "    contours, hierarchy = cv2.findContours(canny, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    # Keeping only the largest detected contour.\n",
    "    page = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n",
    " \n",
    "    # Detecting Edges through Contour approximation.\n",
    "    # Loop over the contours.\n",
    "    if len(page) == 0:\n",
    "        return orig_img\n",
    "    for c in page:\n",
    "        # Approximate the contour.\n",
    "        epsilon = 0.02 * cv2.arcLength(c, True)\n",
    "        corners = cv2.approxPolyDP(c, epsilon, True)\n",
    "        # If our approximated contour has four points.\n",
    "        if len(corners) == 4:\n",
    "            break\n",
    "    # Sorting the corners and converting them to desired shape.\n",
    "    corners = sorted(np.concatenate(corners).tolist())\n",
    "    # For 4 corner points being detected.\n",
    "    corners = order_points(corners)\n",
    " \n",
    "    destination_corners = find_dest(corners)\n",
    " \n",
    "    h, w = orig_img.shape[:2]\n",
    "    # Getting the homography.\n",
    "    M = cv2.getPerspectiveTransform(np.float32(corners), np.float32(destination_corners))\n",
    "    # Perspective transform using homography.\n",
    "    final = cv2.warpPerspective(orig_img, M, (destination_corners[2][0], destination_corners[2][1]),flags=cv2.INTER_LINEAR)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f90e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    '''Rearrange coordinates to order:\n",
    "      top-left, top-right, bottom-right, bottom-left'''\n",
    "    rect = np.zeros((4, 2), dtype='float32')\n",
    "    pts = np.array(pts)\n",
    "    s = pts.sum(axis=1)\n",
    "    # Top-left point will have the smallest sum.\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    # Bottom-right point will have the largest sum.\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    " \n",
    "    diff = np.diff(pts, axis=1)\n",
    "    # Top-right point will have the smallest difference.\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    # Bottom-left will have the largest difference.\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    # return the ordered coordinates\n",
    "    return rect.astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ae73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dest(pts):\n",
    "    (tl, tr, br, bl) = pts\n",
    "    # Finding the maximum width.\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    " \n",
    "    # Finding the maximum height.\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    # Final destination co-ordinates.\n",
    "    destination_corners = [[0, 0], [maxWidth, 0], [maxWidth, maxHeight], [0, maxHeight]]\n",
    " \n",
    "    return order_points(destination_corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eca8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(box,xscale,yscale,xmax,ymax):\n",
    "    box[0]*=1-xscale\n",
    "    box[1]*=1-yscale\n",
    "    box[2]*=1+xscale\n",
    "    box[3]*=1+yscale\n",
    "    if box[2]>xmax:\n",
    "        box[2] = xmax\n",
    "    if box[3]>ymax:\n",
    "        box[3] = ymax\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ba0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_detection(image):\n",
    "    width, height = image.size\n",
    "    feature_extractor = DetrFeatureExtractor()\n",
    "    encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "    model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    results = feature_extractor.post_process_object_detection(outputs, threshold=0.7, target_sizes=[(height, width)])[0]\n",
    "\n",
    "    cropped_img = []\n",
    "    scale = 0.1\n",
    "    all_boxes = []\n",
    "    for i in range(len(results['scores'])):\n",
    "        bounding_box = scaler(results['boxes'][i].tolist(),scale,scale,width,height)\n",
    "        all_boxes.append(bounding_box)\n",
    "        cropped_img.append(image.crop(bounding_box))\n",
    "    \n",
    "    for i in all_boxes:\n",
    "        mask_height = int(i[3] - i[1])\n",
    "        mask_width = int(i[2] - i[0])\n",
    "        square = np.full((mask_height, mask_width),255)\n",
    "        square_img = Image.fromarray(square.astype(np.uint8))\n",
    "        image.paste(square_img,(int(i[0]),int(i[1])))\n",
    "        \n",
    "    return cropped_img,image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6310a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_structure_detection(image):\n",
    "    width, height = image.size\n",
    "    feature_extractor = DetrFeatureExtractor()\n",
    "    encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "    model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-structure-recognition\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    target_sizes = [image.size[::-1]]\n",
    "    results = feature_extractor.post_process_object_detection(outputs, threshold=0.7, target_sizes=target_sizes)[0]\n",
    "    \n",
    "    output = {\"headers\":[],\"row_data\":[]}\n",
    "    y_scale = 0.03\n",
    "    \n",
    "    #labels == 1 is col_data, 2 is row_data, 3 is col_header_data, 4 is row_header_data\n",
    "    threads = []\n",
    "    result = []\n",
    "    lock = threading.Lock()\n",
    "    for i in range(len(results['boxes'])):\n",
    "        if results['labels'][i] == 4:\n",
    "            bounding_box = scaler(results['boxes'][i].tolist(),1,y_scale,width,height)\n",
    "            row_header_img = image.crop(bounding_box)\n",
    "            np_img = np.asarray(row_header_img)\n",
    "            result = ocr_model.ocr(np_img)\n",
    "            for i in result[0]:\n",
    "                output[\"headers\"].append(i[1][0])\n",
    "        elif results['labels'][i] == 2:\n",
    "            bounding_box = scaler(results['boxes'][i].tolist(),1,y_scale,width,height)\n",
    "            row_img = image.crop(bounding_box)\n",
    "            np_img = np.asarray(row_img)\n",
    "            threads.append(threading.Thread(target=ocr_thread, args=(np_img,result,lock)))\n",
    "    \n",
    "    for i in threads:\n",
    "        i.start()\n",
    "    for i in threads:\n",
    "        i.join()\n",
    "        \n",
    "    for i in result:\n",
    "        row_entry = []\n",
    "        for j in i[0]:\n",
    "            row_entry.append(j[1][0])\n",
    "        output[\"row_data\"].append(row_entry)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcae60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_thread(np_img,result,lock):\n",
    "    data = ocr_model.ocr(np_img)\n",
    "    lock.acquire()\n",
    "    result.append(data)\n",
    "    lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc89180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocr_no_temp(img_path,edge_detect=False):\n",
    "    if edge_detect:\n",
    "        #Peform edge detection\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        orig_img = cv2.imread(img_path)\n",
    "        output = scan(img,orig_img)\n",
    "        show(output)\n",
    "        img_path = img_path[:-4]+ \"_cropped\"+img_path[-4:]\n",
    "        cv2.imwrite(img_path,output)\n",
    "        \n",
    "    #Search for table\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    table,image_no_table = table_detection(image)\n",
    "    \n",
    "    #Extract unorganized data\n",
    "    unorganized_data = np.asarray(image_no_table)\n",
    "    result = ocr_model.ocr(unorganized_data)\n",
    "    data_ls = []\n",
    "    for i in result[0]:\n",
    "        data_ls.append(i[1][0])\n",
    "    \n",
    "    extracted_data = {\"unorganized_data\":data_ls,\"Table_Data\":{\"headers\":[],\"row_data\":[],}}\n",
    "    \n",
    "    #Extract Table Data\n",
    "    table_output = []\n",
    "    for i in table:\n",
    "        table_output.append(table_structure_detection(i))\n",
    "    \n",
    "    #Get possible header info\n",
    "    for n in range(len(table_output)):\n",
    "        if len(table_output[n][\"headers\"]) == 0:\n",
    "            for i in table_output[n][\"row_data\"]:\n",
    "                data_str = (\",\").join(i)\n",
    "                if not any(j in data_str for j in '1234567890'):\n",
    "                    table_output[n][\"headers\"].append(i)\n",
    "                    table_output[n][\"row_data\"].remove(i)\n",
    "\n",
    "    #Join Data\n",
    "    for n in range(len(table_output)):\n",
    "        extracted_data[\"Table_Data\"][\"headers\"].append(table_output[n][\"headers\"])\n",
    "        extracted_data[\"Table_Data\"][\"row_data\"].append(table_output[n][\"row_data\"])\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f98cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179bca8321fb4f689efb00d3071b7572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1800f5845efb476d9661e0dbc0bad92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1), <i8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2992\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2992\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   2993\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1), '<i8')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/invoice_sample.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m run_ocr_no_temp(img_path,edge_detect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mrun_ocr_no_temp\u001b[0;34m(img_path, edge_detect)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m#Search for table\u001b[39;00m\n\u001b[1;32m     12\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m table,image_no_table \u001b[39m=\u001b[39m table_detection(image)\n\u001b[1;32m     15\u001b[0m \u001b[39m#Extract unorganized data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m unorganized_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(image_no_table)\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mtable_detection\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     22\u001b[0m     mask_width \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(i[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m i[\u001b[39m0\u001b[39m])\n\u001b[1;32m     23\u001b[0m     square \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((mask_height, mask_width),\u001b[39m255\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     square_img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(square)\n\u001b[1;32m     25\u001b[0m     image\u001b[39m.\u001b[39mpaste(square_img,(\u001b[39mint\u001b[39m(i[\u001b[39m0\u001b[39m]),\u001b[39mint\u001b[39m(i[\u001b[39m1\u001b[39m])))\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m cropped_img,image\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2994\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2992\u001b[0m         mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   2993\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 2994\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   2995\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2996\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1), <i8"
     ]
    }
   ],
   "source": [
    "img_path = \"../data/invoice_sample.jpg\"\n",
    "run_ocr_no_temp(img_path,edge_detect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea2fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
